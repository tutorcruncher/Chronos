import asyncio
import gc
import json
from contextlib import asynccontextmanager
from datetime import UTC, datetime, timedelta

import httpx
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from celery.app import Celery
from fastapi import APIRouter, FastAPI
from httpx import AsyncClient, Limits
from redis import Redis
from sqlalchemy import delete, func
from sqlmodel import Session, select

from chronos.db import engine
from chronos.pydantic_schema import RequestData
from chronos.sql_models import WebhookEndpoint, WebhookLog
from chronos.utils import app_logger, settings

cronjob = APIRouter()

celery_app = Celery(__name__, broker=settings.redis_url, backend=settings.redis_url)
celery_app.conf.broker_connection_retry_on_startup = True
cache = Redis.from_url(settings.redis_url)

# Configure connection pooling
limits = Limits(max_connections=250, max_keepalive_connections=50, keepalive_expiry=60)


@celery_app.task
def ping():
    return 'pong'


async def webhook_request(client: AsyncClient, url: str, endpoint_id: int, *, webhook_sig: str, data: dict = None):
    """
    Send a request to TutorCruncher with retry logic and connection pooling
    :param client: AsyncClient instance
    :param url: The endpoint supplied by clients when creating an integration in TC2
    :param endpoint_id: ID of the webhook endpoint
    :param webhook_sig: The signature generated by hashing the payload with the shared key
    :param data: The Webhook data supplied by TC2
    :return: WebhookEndpoint response
    """
    from chronos.main import logfire

    headers = {
        'User-Agent': 'TutorCruncher',
        'Content-Type': 'application/json',
        'webhook-signature': webhook_sig,
    }

    with logfire.span('{method=} {url!r}', url=url, method='POST'):
        r = None
        max_retries = 3
        retry_delay = 0.5  # Initial delay in seconds
        last_error = None

        for attempt in range(max_retries):
            try:
                r = await client.post(url=url, json=data, headers=headers, timeout=settings.webhook_timeout)
                if r.status_code in [500, 502, 503, 504]:
                    last_error = f'Server error with status code {r.status_code}'
                    if attempt < max_retries - 1:
                        app_logger.info(
                            'Retrying request to %s after status code %s (attempt %s/%s)',
                            url,
                            r.status_code,
                            attempt + 1,
                            max_retries,
                        )
                        await asyncio.sleep(retry_delay * (2**attempt))  # Exponential backoff
                        continue
                break
            except httpx.TimeoutException as terr:
                last_error = terr
                if attempt < max_retries - 1:
                    app_logger.info(
                        'Timeout error sending webhook to %s: %s (attempt %s/%s)', url, terr, attempt + 1, max_retries
                    )
                    await asyncio.sleep(retry_delay * (2**attempt))
                    continue
                app_logger.info('Final timeout error sending webhook to %s: %s', url, terr)
            except httpx.HTTPError as httperr:
                last_error = httperr
                if attempt < max_retries - 1:
                    app_logger.info(
                        'HTTP error sending webhook to %s: %s (attempt %s/%s)', url, httperr, attempt + 1, max_retries
                    )
                    await asyncio.sleep(retry_delay * (2**attempt))
                    continue
                app_logger.info('Final HTTP error sending webhook to %s: %s', url, httperr)

    request_data = RequestData(
        endpoint_id=endpoint_id, request_headers=json.dumps(headers), request_body=json.dumps(data)
    )

    if r is not None and r.status_code not in [500, 502, 503, 504]:
        request_data.response_headers = json.dumps(dict(r.headers))
        request_data.response_body = r.content.decode()
        request_data.status_code = r.status_code
        request_data.successful_response = True
    else:
        # Handle the case where the request failed completely
        error_message = str(last_error) if last_error else 'No response from endpoint'
        request_data.response_headers = json.dumps({'Message': error_message})
        request_data.response_body = json.dumps({'Message': error_message})
        request_data.status_code = r.status_code if r is not None else 999
        request_data.successful_response = False

    return request_data


acceptable_url_schemes = ('http', 'https', 'ftp', 'ftps')


def get_qlength():
    """
    Get the length of the queue from celery. Celery returns a dictionary like so: {'queue_name': [task1, task2, ...]}
    so to get qlength we simply aggregate the length of all task lists
    """
    qlength = 0
    celery_inspector = celery_app.control.inspect()
    dict_of_queues = celery_inspector.reserved()
    if dict_of_queues and isinstance(dict_of_queues, dict):
        for k, v in dict_of_queues.items():
            qlength += len(v)

    return qlength


def _get_webhook_headers() -> dict:
    return {
        'User-Agent': 'TutorCruncher',
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {settings.tc2_shared_key}',
    }


async def _try_send_webhook(client, endpoint, loaded_payload, attempt=1, max_attempts=3):
    try:
        response = await client.post(
            endpoint.webhook_url,
            json=loaded_payload,
            headers=_get_webhook_headers(),
            timeout=settings.webhook_timeout,
        )

        if response.status_code in {200, 201, 202, 204}:
            app_logger.info('Webhook request successful for endpoint %s', endpoint.id)
            return response, 'Success'
        elif response.status_code in {500, 502, 503, 504} and attempt < max_attempts:
            app_logger.info('Retrying webhook for endpoint %s (attempt %s/%s)', endpoint.id, attempt, max_attempts)
            await asyncio.sleep(2 ** (attempt - 1))  # Exponential backoff
            return await _try_send_webhook(client, endpoint, loaded_payload, attempt + 1)
        else:
            app_logger.info('Webhook request failed for endpoint %s with status %s', endpoint.id, response.status_code)
            return response, 'Failed'
    except (httpx.RequestError, httpx.TimeoutException) as e:
        if attempt < max_attempts:
            app_logger.info('Retrying webhook for endpoint %s (attempt %s/%s)', endpoint.id, attempt, max_attempts)
            await asyncio.sleep(2 ** (attempt - 1))  # Exponential backoff
            return await _try_send_webhook(client, endpoint, loaded_payload, attempt + 1)
        app_logger.error('Error sending webhook to %s: %s', endpoint.webhook_url, str(e))
        return None, 'Failed'


async def _async_post_webhooks(endpoints, url_extension, payload):
    webhook_logs = []
    total_success, total_failed = 0, 0

    try:
        loaded_payload = json.loads(payload)
        if isinstance(loaded_payload, str):
            loaded_payload = json.loads(loaded_payload)
    except json.JSONDecodeError:
        app_logger.error('Invalid JSON payload: %s', payload)
        return [], total_success, total_failed

    if not isinstance(loaded_payload, dict):
        app_logger.error('Invalid payload format: expected dictionary')
        return [], total_success, total_failed

    if loaded_payload == {}:
        app_logger.error('Empty payload')
        return [], total_success, total_failed

    # Filter endpoints based on branch_id if present in events
    target_branch_ids = set()
    if 'events' in loaded_payload:
        if not isinstance(loaded_payload['events'], list):
            app_logger.error('Invalid events format: expected array')
            return [], total_success, total_failed
        for event in loaded_payload['events']:
            if isinstance(event, dict) and 'branch' in event:
                target_branch_ids.add(event['branch'])
        if not target_branch_ids:  # If no valid branch IDs found in events, use default
            target_branch_ids.add(99)
    elif 'branch_id' in loaded_payload:
        target_branch_ids.add(loaded_payload['branch_id'])
    else:
        # If no branch_id is specified, only process endpoints with branch_id 99 (default)
        target_branch_ids.add(99)  # Default branch_id

    # Process all endpoints that match any target branch_id
    # Use a dictionary to ensure each endpoint is only processed once
    filtered_endpoints = {}
    for endpoint in endpoints:
        if endpoint.branch_id in target_branch_ids:
            filtered_endpoints[endpoint.id] = endpoint

    if not filtered_endpoints:
        app_logger.info('No matching endpoints found for branch_ids: %s', target_branch_ids)
        return [], total_success, total_failed

    # Create a single client for all requests to enable connection pooling
    async with httpx.AsyncClient(limits=limits) as client:
        tasks = []
        active_endpoints = []
        for endpoint in filtered_endpoints.values():
            if not endpoint.active:
                app_logger.info('Skipping inactive endpoint: %s', endpoint.id)
                continue

            # Check if the webhook URL is valid
            if not endpoint.webhook_url or not any(
                endpoint.webhook_url.startswith(scheme) for scheme in acceptable_url_schemes
            ):
                app_logger.error(
                    'Webhook URL does not start with an acceptable url scheme: %s (%s)',
                    endpoint.webhook_url,
                    endpoint.id,
                )
                continue

            tasks.append(_try_send_webhook(client, endpoint, loaded_payload))
            active_endpoints.append(endpoint)

        # Run all tasks concurrently
        results = await asyncio.gather(*tasks)

        for endpoint, result in zip(active_endpoints, results):
            response, status = result
            if status == 'Success':
                total_success += 1
                app_logger.info('Successfully sent webhook to endpoint %s', endpoint.id)
            else:
                total_failed += 1
                app_logger.info('Failed to send webhook to endpoint %s', endpoint.id)

            webhook_logs.append(
                WebhookLog(
                    webhook_endpoint_id=endpoint.id,
                    request_headers=json.dumps(_get_webhook_headers()),
                    request_body=json.dumps(loaded_payload),
                    response_headers=json.dumps(dict(response.headers) if response else {}),
                    response_body=response.text if response else str(response),
                    status=status,
                    status_code=response.status_code if response else 0,
                    timestamp=datetime.utcnow(),
                )
            )

    return webhook_logs, total_success, total_failed


@celery_app.task
def task_send_webhooks(
    payload: str,
    url_extension: str = None,
):
    """
    Send the webhook to the relevant endpoints
    """
    try:
        loaded_payload = json.loads(payload)
        if isinstance(loaded_payload, str):
            loaded_payload = json.loads(loaded_payload)
    except json.JSONDecodeError:
        app_logger.error('Invalid JSON payload: %s', payload)
        return

    # Handle request_time field if present
    if '_request_time' in loaded_payload:
        loaded_payload['request_time'] = loaded_payload.pop('_request_time')

    with Session(engine) as db:
        endpoints = db.exec(select(WebhookEndpoint)).all()
        webhook_logs, total_success, total_failed = asyncio.run(_async_post_webhooks(endpoints, url_extension, payload))
        if webhook_logs:
            for log in webhook_logs:
                db.add(log)
            db.commit()

    app_logger.info(
        '%s Webhooks sent. Total Sent: %s. Total failed: %s',
        total_success + total_failed,
        total_success,
        total_failed,
    )
    return total_success + total_failed


DELETE_JOBS_KEY = 'delete_old_logs_job'

scheduler = AsyncIOScheduler(timezone=UTC)


@asynccontextmanager
async def lifespan(app: FastAPI):
    scheduler.start()
    yield
    scheduler.shutdown()


@scheduler.scheduled_job('interval', hours=1)
async def delete_old_logs_job():
    """
    We run cron job at midnight every day that wipes all WebhookLogs older than 15 days
    """
    if cache.get(DELETE_JOBS_KEY):
        return
    else:
        cache.set(DELETE_JOBS_KEY, 'True', ex=1200)
        _delete_old_logs_job.delay()


def get_count(date_to_delete_before: datetime) -> int:
    """
    Get the count of all logs
    """
    with Session(engine) as db:
        count = (
            db.query(WebhookLog)
            .with_entities(func.count())
            .where(WebhookLog.timestamp < date_to_delete_before)  # Use < to keep exactly 15-day-old logs
            .scalar()
        )
    return count


@celery_app.task
def _delete_old_logs_job():
    """Delete webhook logs older than 15 days"""
    with Session(engine) as db:
        date_to_delete_before = datetime.now(UTC) - timedelta(days=15)
        # Add a small buffer to ensure we keep logs that are exactly 15 days old
        date_to_delete_before = date_to_delete_before.replace(microsecond=0)
        count = get_count(date_to_delete_before)
        if count > 0:
            app_logger.info('Deleting %d webhook logs older than %s', count, date_to_delete_before)
            stmt = delete(WebhookLog).where(
                WebhookLog.timestamp < date_to_delete_before
            )  # Use < to keep exactly 15-day-old logs
            db.execute(stmt)
            db.commit()
            gc.collect()  # Force garbage collection after large delete
        else:
            app_logger.info('No webhook logs to delete')

    cache.delete(DELETE_JOBS_KEY)
