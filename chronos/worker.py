import asyncio
import copy
import gc
import hashlib
import hmac
import json
import time
from contextlib import asynccontextmanager
from datetime import UTC, datetime, timedelta

import httpx
import logfire
import redis.exceptions
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from celery.app import Celery
from celery.signals import worker_process_init
from fastapi import APIRouter, FastAPI
from httpx import AsyncClient
from redis import Redis
from sqlalchemy import delete, func
from sqlmodel import Session, select

from chronos.db import engine
from chronos.pydantic_schema import RequestData
from chronos.sql_models import WebhookEndpoint, WebhookLog
from chronos.tasks.queue import JobQueue
from chronos.utils import app_logger, settings

cronjob = APIRouter()

celery_app = Celery(__name__, broker=settings.redis_url, backend=settings.redis_url)
celery_app.conf.update(
    broker_connection_retry_on_startup=True,
    # Serialization
    task_serializer='json',
    accept_content=['json'],
    result_serializer='json',
    # Reliability: ack after completion, requeue on crash
    task_acks_late=True,
    task_reject_on_worker_lost=True,
    # Worker tuning: fetch one task at a time for fair scheduling
    worker_prefetch_multiplier=1,
    # Task execution limits
    task_soft_time_limit=300,
    task_time_limit=600,
    # Route dispatcher to its own queue
    task_routes={
        'job_dispatcher': {'queue': 'dispatcher'},
    },
)

if settings.testing:
    celery_app.conf.update(task_always_eager=True, task_eager_propagates=True)

cache = Redis.from_url(settings.redis_url)

# Initialize job queue with the same Redis client
job_queue = JobQueue(redis_client=cache)


@worker_process_init.connect
def init_worker_process(**kwargs):
    """
    Dispose of the database connection pool when a worker process is forked.
    This ensures each worker gets fresh connections instead of inheriting
    stale connections from the parent process, preventing SSL SYSCALL errors.
    """
    app_logger.info('Disposing database engine pool for worker process')
    engine.dispose()


async def webhook_request(client: AsyncClient, url: str, endpoint_id: int, *, webhook_sig: str, data: dict = None):
    """
    Send a request to TutorCruncher
    :param client
    :param url: The endpoint supplied by clients when creating an integration in TC2
    :param method: We should always be sending POST requests as we are sending data to the endpoints
    :param webhook_sig: The signature generated by hashing the payload with the shared key
    :param data: The Webhook data supplied by TC2
    :return: WebhookEndpoint response
    """
    from chronos.main import logfire

    headers = {
        'User-Agent': 'TutorCruncher',
        'Content-Type': 'application/json',
        'webhook-signature': webhook_sig,
    }
    with logfire.span('{method=} {url!r}', url=url, method='POST'):
        r = None
        try:
            r = await client.post(url=url, json=data, headers=headers, timeout=8)
        except httpx.TimeoutException as terr:
            app_logger.info('Timeout error sending webhook to %s: %s', url, terr)
        except httpx.HTTPError as httperr:
            app_logger.info('HTTP error sending webhook to %s: %s', url, httperr)
    request_data = RequestData(
        endpoint_id=endpoint_id, request_headers=json.dumps(headers), request_body=json.dumps(data)
    )
    if r is not None:
        request_data.response_headers = json.dumps(dict(r.headers))
        request_data.response_body = json.dumps(r.content.decode())
        request_data.status_code = r.status_code
        request_data.successful_response = True
    return request_data


acceptable_url_schemes = ('http', 'https', 'ftp', 'ftps')


# def get_qlength():
#     """
#     Get the length of the queue from celery. Celery returns a dictionary like so: {'queue_name': [task1, task2, ...]}
#     so to get qlength we simply aggregate the length of all task lists
#     """
#     qlength = 0
#     celery_inspector = celery_app.control.inspect()
#     dict_of_queues = celery_inspector.reserved()
#     if dict_of_queues and isinstance(dict_of_queues, dict):
#         for k, v in dict_of_queues.items():
#             qlength += len(v)
#
#     return qlength


async def _async_post_webhooks(endpoints, url_extension, payload):
    webhook_logs = []
    total_success, total_failed = 0, 0
    # Temporary fix for the issue with the number of connections caused by a certain client
    limits = httpx.Limits(max_connections=250)
    loaded_payload = json.loads(payload)

    async with AsyncClient(limits=limits) as client:
        tasks = []
        task_endpoints = []
        for endpoint in endpoints:
            # Check if the webhook URL is valid
            if not endpoint.webhook_url.startswith(acceptable_url_schemes):
                app_logger.error(
                    'Webhook URL does not start with an acceptable url scheme: %s (%s)',
                    endpoint.webhook_url,
                    endpoint.id,
                )
                continue
            url = endpoint.webhook_url
            if url_extension:
                url += f'/{url_extension}'

            events = loaded_payload.get('events')
            if events is not None:
                payloads_to_send = []
                for event in events:
                    # for batched events requests from TC2 we want to split each action event
                    # into their individual requests to the downstream endpoint. This is done for reverse compability
                    # with zapier and other client integration
                    single_event_payload = copy.deepcopy({k: v for k, v in loaded_payload.items() if k != 'events'})
                    single_event_payload['events'] = [copy.deepcopy(event)]
                    payloads_to_send.append(single_event_payload)
            else:
                payloads_to_send = [copy.deepcopy(loaded_payload)]
            for payload_to_send in payloads_to_send:
                send_json = json.dumps(payload_to_send)
                sig = hmac.new(endpoint.api_key.encode(), send_json.encode(), hashlib.sha256)
                task = asyncio.ensure_future(
                    webhook_request(client, url, endpoint.id, webhook_sig=sig.hexdigest(), data=payload_to_send)
                )
                tasks.append(task)
                task_endpoints.append((endpoint.id, url))
        webhook_responses = await asyncio.gather(*tasks, return_exceptions=True)
        # webhook responses and tasks endpoints are in task creation order.
        # zip gives the matching endpoint id url for each result, including exceptions.
        for response, (endpoint_id, endpoint_url) in zip(webhook_responses, task_endpoints):
            if not isinstance(response, RequestData):
                app_logger.info('No response from endpoint %s: %s. %s', endpoint_id, endpoint_url, response)
                continue
            elif not response.successful_response:
                app_logger.info('No response from endpoint %s: %s', response.endpoint_id, endpoint_url)

            if response.status_code in {200, 201, 202, 204}:
                status = 'Success'
                total_success += 1
            else:
                status = 'Unexpected response'
                total_failed += 1

            # Log the response
            webhook_logs.append(
                WebhookLog(
                    webhook_endpoint_id=response.endpoint_id,
                    request_headers=response.request_headers,
                    request_body=response.request_body,
                    response_headers=response.response_headers,
                    response_body=response.response_body,
                    status=status,
                    status_code=response.status_code,
                )
            )
    return webhook_logs, total_success, total_failed


@celery_app.task(
    autoretry_for=(redis.exceptions.ConnectionError, redis.exceptions.TimeoutError),
    retry_kwargs={'max_retries': 3},
    retry_backoff=True,
)
def task_send_webhooks(
    payload: str,
    url_extension: str = None,
):
    """
    Send the webhook to the relevant endpoints
    """
    loaded_payload = json.loads(payload)
    loaded_payload['_request_time'] = loaded_payload.pop('request_time')

    if loaded_payload.get('events'):
        branch_id = loaded_payload['events'][0]['branch']
    else:
        branch_id = loaded_payload['branch_id']

    qlength = job_queue.get_celery_queue_length()
    if qlength > 100:
        app_logger.error('Queue is too long, qlength=%s. Check workers and speeds.', qlength)

    app_logger.info('Starting send webhook task for branch %s. qlength=%s.', branch_id, qlength)
    lf_span = 'Sending webhooks for branch: {branch_id=}'
    with logfire.span(lf_span, branch_id=branch_id):
        with Session(engine) as db:
            # Get all the endpoints for the branch
            endpoints_query = select(WebhookEndpoint).where(
                WebhookEndpoint.branch_id == branch_id, WebhookEndpoint.active
            )
            endpoints = db.exec(endpoints_query).all()

            webhook_logs, total_success, total_failed = asyncio.run(
                _async_post_webhooks(endpoints, url_extension, payload)
            )
            for webhook_log in webhook_logs:
                db.add(webhook_log)
            db.commit()
    app_logger.info(
        '%s Webhooks sent for branch %s. Total Sent: %s. Total failed: %s',
        total_success + total_failed,
        branch_id,
        total_success,
        total_failed,
    )


DELETE_JOBS_KEY = 'delete_old_logs_job'

scheduler = AsyncIOScheduler(timezone=UTC)


@asynccontextmanager
async def lifespan(app: FastAPI):
    scheduler.start()
    yield
    scheduler.shutdown()


@scheduler.scheduled_job('interval', hours=1)
async def delete_old_logs_job():
    """
    We run cron job at midnight every day that wipes all WebhookLogs older than 15 days
    """
    if cache.get(DELETE_JOBS_KEY):
        return
    else:
        cache.set(DELETE_JOBS_KEY, 'True', ex=1200)
        _delete_old_logs_job.delay()


def get_count(date_to_delete_before: datetime) -> int:
    """
    Get the count of all logs
    """
    with Session(engine) as db:
        count = (
            db.query(WebhookLog)
            .with_entities(func.count())
            .where(WebhookLog.timestamp < date_to_delete_before)
            .scalar()
        )
    return count


@celery_app.task
def _delete_old_logs_job():
    # with logfire.span('Started to delete old logs'):
    with Session(engine) as db:
        # Get all logs older than 15 days
        date_to_delete_before = datetime.now(UTC) - timedelta(days=15)
        count = get_count(date_to_delete_before)
        delete_limit = 4999
        while count > 0:
            app_logger.info(f'Deleting {count} logs')
            logs_to_delete = db.exec(
                select(WebhookLog.id).where(WebhookLog.timestamp < date_to_delete_before).limit(delete_limit)
            ).all()
            delete_statement = delete(WebhookLog).where(WebhookLog.id.in_(log_id for log_id in logs_to_delete))
            db.exec(delete_statement)
            db.commit()
            count -= delete_limit

            del logs_to_delete
            del delete_statement
            gc.collect()

    cache.delete(DELETE_JOBS_KEY)


GLOBAL_BRANCH_ID = 0


def dispatch_branch_task(task, routing_branch_id: int, **kwargs) -> None:
    """Dispatch a task to per-branch Redis queue for round-robin processing.

    In eager mode (testing), tasks are executed synchronously bypassing Redis.
    """
    if celery_app.conf.task_always_eager:
        task.apply_async(kwargs=kwargs)
    else:
        job_queue.enqueue(task.name, routing_branch_id=routing_branch_id, **kwargs)


@celery_app.task(name='job_dispatcher', acks_late=False)
def job_dispatcher_task(
    max_celery_queue: int = 50,
    cycle_delay: float = 0.01,
    idle_delay: float = 1.0,
) -> None:
    """Celery task that runs the continuous round-robin dispatcher.

    Runs indefinitely, dispatching jobs from per-branch queues to Celery workers.
    Implements backpressure by pausing when the Celery queue is full.

    CRITICAL: acks_late=False overrides the global task_acks_late=True.
    The dispatcher runs forever and never completes. With acks_late=True,
    the Redis broker's visibility_timeout (default 1 hour) would redeliver
    the unacked task, spawning a DUPLICATE dispatcher. Setting acks_late=False
    acks the task immediately on receipt, preventing redelivery.
    """
    from billiard.exceptions import SoftTimeLimitExceeded

    from chronos.tasks.dispatcher import dispatch_cycle

    app_logger.info('Job dispatcher started')
    while True:
        try:
            if not job_queue.has_active_jobs():
                time.sleep(idle_delay)
                continue

            # LLEN celery: measures pending broker queue only, not in-flight tasks.
            # Acceptable overshoot bounded by batch_limit. See Edge Case 26.
            celery_queue_len = job_queue.get_celery_queue_length()
            if celery_queue_len >= max_celery_queue:
                time.sleep(cycle_delay)
                continue

            with logfire.span('Dispatching jobs') as span:
                dispatched = dispatch_cycle()
                if dispatched > 0:
                    span.message = f'Dispatched {dispatched} jobs'
                    app_logger.debug('Dispatched %d jobs', dispatched)

            time.sleep(cycle_delay)
        except SoftTimeLimitExceeded:
            # Safety net: if CLI flags (--soft-time-limit=0) aren't set properly,
            # catch the exception and continue rather than dying.
            app_logger.warning('Dispatcher caught SoftTimeLimitExceeded, continuing')
            continue
        except Exception:
            # CRITICAL: Catch all other exceptions to prevent the dispatcher from
            # dying permanently. dispatch_cycle() or job_queue methods can raise
            # redis.ConnectionError, serialization errors, broker hiccups, etc.
            # The worker_ready signal only fires on worker process start, NOT on
            # task failure â€” so an uncaught exception here kills the dispatcher
            # with no automatic recovery until the worker process itself restarts.
            app_logger.exception('Dispatcher error, sleeping %s seconds before retry', idle_delay)
            time.sleep(idle_delay)
            continue


# Import worker_startup to register the signal handler.
# Must be at the bottom of the file because worker_startup imports from this module.
import chronos.tasks.worker_startup  # noqa: F401, E402
