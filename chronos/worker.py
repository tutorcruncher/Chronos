import asyncio
import gc
import json
from contextlib import asynccontextmanager
from datetime import UTC, datetime, timedelta

import httpx
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from celery.app import Celery
from fastapi import APIRouter, FastAPI
from httpx import AsyncClient, Limits
from redis import Redis
from sqlalchemy import delete, func
from sqlmodel import Session, select

from chronos.db import engine
from chronos.pydantic_schema import RequestData
from chronos.sql_models import WebhookEndpoint, WebhookLog
from chronos.utils import app_logger, settings

cronjob = APIRouter()

celery_app = Celery(__name__, broker=settings.redis_url, backend=settings.redis_url)
celery_app.conf.broker_connection_retry_on_startup = True
cache = Redis.from_url(settings.redis_url)

# Configure connection pooling
limits = Limits(max_connections=250, max_keepalive_connections=50, keepalive_expiry=60)


@celery_app.task
def ping():
    return 'pong'


async def webhook_request(client: AsyncClient, url: str, endpoint_id: int, *, webhook_sig: str, data: dict = None):
    """
    Send a request to TutorCruncher with retry logic and connection pooling
    :param client: AsyncClient instance
    :param url: The endpoint supplied by clients when creating an integration in TC2
    :param endpoint_id: ID of the webhook endpoint
    :param webhook_sig: The signature generated by hashing the payload with the shared key
    :param data: The Webhook data supplied by TC2
    :return: WebhookEndpoint response
    """
    from chronos.main import logfire

    headers = {
        'User-Agent': 'TutorCruncher',
        'Content-Type': 'application/json',
        'webhook-signature': webhook_sig,
    }

    with logfire.span('{method=} {url!r}', url=url, method='POST'):
        r = None
        max_retries = 3
        retry_delay = 0.5  # Initial delay in seconds
        last_error = None

        for attempt in range(max_retries):
            try:
                r = await client.post(url=url, json=data, headers=headers, timeout=settings.webhook_timeout)
                if r.status_code in [500, 502, 503, 504]:
                    last_error = f'Server error with status code {r.status_code}'
                    if attempt < max_retries - 1:
                        app_logger.info(
                            'Retrying request to %s after status code %s (attempt %s/%s)',
                            url,
                            r.status_code,
                            attempt + 1,
                            max_retries,
                        )
                        await asyncio.sleep(retry_delay * (2**attempt))  # Exponential backoff
                        continue
                break
            except httpx.TimeoutException as terr:
                last_error = terr
                if attempt < max_retries - 1:
                    app_logger.info(
                        'Timeout error sending webhook to %s: %s (attempt %s/%s)', url, terr, attempt + 1, max_retries
                    )
                    await asyncio.sleep(retry_delay * (2**attempt))
                    continue
                app_logger.info('Final timeout error sending webhook to %s: %s', url, terr)
            except httpx.HTTPError as httperr:
                last_error = httperr
                if attempt < max_retries - 1:
                    app_logger.info(
                        'HTTP error sending webhook to %s: %s (attempt %s/%s)', url, httperr, attempt + 1, max_retries
                    )
                    await asyncio.sleep(retry_delay * (2**attempt))
                    continue
                app_logger.info('Final HTTP error sending webhook to %s: %s', url, httperr)

    request_data = RequestData(
        endpoint_id=endpoint_id, request_headers=json.dumps(headers), request_body=json.dumps(data)
    )

    if r is not None and r.status_code not in [500, 502, 503, 504]:
        request_data.response_headers = json.dumps(dict(r.headers))
        request_data.response_body = r.content.decode()
        request_data.status_code = r.status_code
        request_data.successful_response = True
    else:
        # Handle the case where the request failed completely
        error_message = str(last_error) if last_error else 'No response from endpoint'
        request_data.response_headers = json.dumps({'Message': error_message})
        request_data.response_body = json.dumps({'Message': error_message})
        request_data.status_code = r.status_code if r is not None else 999
        request_data.successful_response = False

    return request_data


acceptable_url_schemes = ('http', 'https', 'ftp', 'ftps')


def get_qlength():
    """
    Get the length of the queue from celery. Celery returns a dictionary like so: {'queue_name': [task1, task2, ...]}
    so to get qlength we simply aggregate the length of all task lists
    """
    qlength = 0
    celery_inspector = celery_app.control.inspect()
    dict_of_queues = celery_inspector.reserved()
    if dict_of_queues and isinstance(dict_of_queues, dict):
        for k, v in dict_of_queues.items():
            qlength += len(v)

    return qlength


def _get_webhook_headers() -> dict:
    return {
        'User-Agent': 'TutorCruncher',
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {settings.tc2_shared_key}',
    }


async def _try_send_webhook(client, endpoint, loaded_payload, attempt=1, max_attempts=3):
    try:
        response = await client.post(
            endpoint.webhook_url,
            json=loaded_payload,
            headers=_get_webhook_headers(),
            timeout=settings.webhook_timeout,
        )

        if response.status_code in {200, 201, 202, 204}:
            return response, 'Success'
        elif response.status_code in {500, 502, 503, 504} and attempt < max_attempts:
            app_logger.info(f'Retrying webhook for endpoint {endpoint.id} (attempt {attempt})')
            await asyncio.sleep(1)  # Add a small delay between retries
            return await _try_send_webhook(client, endpoint, loaded_payload, attempt + 1)
        else:
            app_logger.info(f'Webhook request failed for endpoint {endpoint.id} with status {response.status_code}')
            return response, 'Failed'
    except (httpx.RequestError, httpx.TimeoutException) as e:
        if attempt < max_attempts:
            app_logger.info(f'Retrying webhook for endpoint {endpoint.id} (attempt {attempt})')
            await asyncio.sleep(1)  # Add a small delay between retries
            return await _try_send_webhook(client, endpoint, loaded_payload, attempt + 1)
        app_logger.error('Error sending webhook to %s: %s', endpoint.webhook_url, str(e))
        return None, 'Failed'


async def _async_post_webhooks(endpoints, url_extension, payload):
    webhook_logs = []
    total_success, total_failed = 0, 0

    try:
        loaded_payload = json.loads(payload)
        if isinstance(loaded_payload, str):
            loaded_payload = json.loads(loaded_payload)
    except json.JSONDecodeError:
        app_logger.error('Invalid JSON payload: %s', payload)
        return webhook_logs, total_success, total_failed

    # Filter endpoints based on branch_id if present in events
    target_branch_ids = set()
    if 'events' in loaded_payload:
        for event in loaded_payload['events']:
            if 'branch' in event:
                target_branch_ids.add(event['branch'])
    elif 'branch_id' in loaded_payload:
        target_branch_ids.add(loaded_payload['branch_id'])

    # If no branch_id is specified, only process endpoints with branch_id 99 (default)
    if not target_branch_ids:
        target_branch_ids.add(99)  # Default branch_id

    # Only process endpoints that match the target branch_ids
    filtered_endpoints = []
    for endpoint in endpoints:
        if endpoint.branch_id in target_branch_ids:
            filtered_endpoints.append(endpoint)
            app_logger.info(f'Processing endpoint {endpoint.id} with branch_id {endpoint.branch_id}')

    async with httpx.AsyncClient(limits=limits) as client:  # Use connection pooling
        for endpoint in filtered_endpoints:
            if not endpoint.active:
                app_logger.info('Skipping inactive endpoint: %s', endpoint.id)
                continue

            # Check if the webhook URL is valid
            if not endpoint.webhook_url or not endpoint.webhook_url.startswith(acceptable_url_schemes):
                app_logger.error(
                    'Webhook URL does not start with an acceptable url scheme: %s (%s)',
                    endpoint.webhook_url,
                    endpoint.id,
                )
                continue

            response, status = await _try_send_webhook(client, endpoint, loaded_payload)

            if status == 'Success':
                total_success += 1
            else:
                total_failed += 1

            webhook_logs.append(
                WebhookLog(
                    webhook_endpoint_id=endpoint.id,
                    request_headers=json.dumps(dict(response.request.headers) if response else {}),
                    request_body=json.dumps(loaded_payload),
                    response_headers=json.dumps(dict(response.headers) if response else {}),
                    response_body=response.text if response else str(response),
                    status=status,
                    status_code=response.status_code if response else 0,
                    timestamp=datetime.utcnow(),
                )
            )

    return webhook_logs, total_success, total_failed


@celery_app.task
def task_send_webhooks(
    payload: str,
    url_extension: str = None,
):
    """
    Send the webhook to the relevant endpoints
    """
    try:
        loaded_payload = json.loads(payload)
        if isinstance(loaded_payload, str):
            loaded_payload = json.loads(loaded_payload)
    except json.JSONDecodeError:
        app_logger.error('Invalid JSON payload: %s', payload)
        return

    # Handle request_time field if present
    if '_request_time' in loaded_payload:
        loaded_payload['request_time'] = loaded_payload.pop('_request_time')

    with Session(engine) as db:
        endpoints = db.exec(select(WebhookEndpoint)).all()
        webhook_logs, total_success, total_failed = asyncio.run(_async_post_webhooks(endpoints, url_extension, payload))
        if webhook_logs:
            for log in webhook_logs:
                db.add(log)
            db.commit()

    app_logger.info(
        '%s Webhooks sent. Total Sent: %s. Total failed: %s',
        total_success + total_failed,
        total_success,
        total_failed,
    )
    return total_success + total_failed


DELETE_JOBS_KEY = 'delete_old_logs_job'

scheduler = AsyncIOScheduler(timezone=UTC)


@asynccontextmanager
async def lifespan(app: FastAPI):
    scheduler.start()
    yield
    scheduler.shutdown()


@scheduler.scheduled_job('interval', hours=1)
async def delete_old_logs_job():
    """
    We run cron job at midnight every day that wipes all WebhookLogs older than 15 days
    """
    if cache.get(DELETE_JOBS_KEY):
        return
    else:
        cache.set(DELETE_JOBS_KEY, 'True', ex=1200)
        _delete_old_logs_job.delay()


def get_count(date_to_delete_before: datetime) -> int:
    """
    Get the count of all logs
    """
    with Session(engine) as db:
        count = (
            db.query(WebhookLog)
            .with_entities(func.count())
            .where(WebhookLog.timestamp < date_to_delete_before)
            .scalar()
        )
    return count


@celery_app.task
def _delete_old_logs_job():
    # with logfire.span('Started to delete old logs'):
    with Session(engine) as db:
        # Get all logs older than 15 days
        date_to_delete_before = datetime.now(UTC) - timedelta(days=15)
        count = get_count(date_to_delete_before)
        delete_limit = 4999
        while count > 0:
            app_logger.info(f'Deleting {count} logs')
            logs_to_delete = db.exec(
                select(WebhookLog.id).where(WebhookLog.timestamp < date_to_delete_before).limit(delete_limit)
            ).all()
            delete_statement = delete(WebhookLog).where(WebhookLog.id.in_(log_id for log_id in logs_to_delete))
            db.exec(delete_statement)
            db.commit()
            count -= delete_limit

            del logs_to_delete
            del delete_statement
            gc.collect()

    cache.delete(DELETE_JOBS_KEY)
