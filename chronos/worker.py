import asyncio
import json
from contextlib import asynccontextmanager
from datetime import UTC, datetime, timedelta

import httpx
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from celery.app import Celery
from fastapi import APIRouter, FastAPI
from httpx import AsyncClient, Limits
from redis import Redis
from sqlalchemy import delete, func
from sqlmodel import Session, select

from chronos.db import engine
from chronos.pydantic_schema import RequestData
from chronos.sql_models import WebhookEndpoint, WebhookLog
from chronos.utils import app_logger, settings

cronjob = APIRouter()

celery_app = Celery(__name__, broker=settings.redis_url, backend=settings.redis_url)
celery_app.conf.broker_connection_retry_on_startup = True
cache = Redis.from_url(settings.redis_url)

# Configure connection pooling
limits = Limits(max_connections=250, max_keepalive_connections=50, keepalive_expiry=60)


@celery_app.task
def ping():
    return 'pong'


async def webhook_request(client: AsyncClient, url: str, endpoint_id: int, *, webhook_sig: str, data: dict = None):
    """
    Send a request to TutorCruncher with retry logic and connection pooling
    :param client: AsyncClient instance
    :param url: The endpoint supplied by clients when creating an integration in TC2
    :param endpoint_id: ID of the webhook endpoint
    :param webhook_sig: The signature generated by hashing the payload with the shared key
    :param data: The Webhook data supplied by TC2
    :return: WebhookEndpoint response
    """
    from chronos.main import logfire

    headers = {
        'User-Agent': 'TutorCruncher',
        'Content-Type': 'application/json',
        'webhook-signature': webhook_sig,
    }

    with logfire.span('{method=} {url!r}', url=url, method='POST'):
        r = None
        max_retries = 3
        retry_delay = 0.5  # Initial delay in seconds
        last_error = None

        for attempt in range(max_retries):
            try:
                r = await client.post(url=url, json=data, headers=headers, timeout=settings.webhook_timeout)
                if r.status_code in [500, 502, 503, 504]:
                    last_error = f'Server error with status code {r.status_code}'
                    if attempt < max_retries - 1:
                        app_logger.info(
                            'Retrying request to %s after status code %s (attempt %s/%s)',
                            url,
                            r.status_code,
                            attempt + 1,
                            max_retries,
                        )
                        await asyncio.sleep(retry_delay * (2**attempt))  # Exponential backoff
                        continue
                break
            except httpx.TimeoutException as terr:
                last_error = terr
                if attempt < max_retries - 1:
                    app_logger.info(
                        'Timeout error sending webhook to %s: %s (attempt %s/%s)', url, terr, attempt + 1, max_retries
                    )
                    await asyncio.sleep(retry_delay * (2**attempt))
                    continue
                app_logger.info('Final timeout error sending webhook to %s: %s', url, terr)
            except httpx.HTTPError as httperr:
                last_error = httperr
                if attempt < max_retries - 1:
                    app_logger.info(
                        'HTTP error sending webhook to %s: %s (attempt %s/%s)', url, httperr, attempt + 1, max_retries
                    )
                    await asyncio.sleep(retry_delay * (2**attempt))
                    continue
                app_logger.info('Final HTTP error sending webhook to %s: %s', url, httperr)

    request_data = RequestData(
        endpoint_id=endpoint_id, request_headers=json.dumps(headers), request_body=json.dumps(data)
    )

    if r is not None and r.status_code not in [500, 502, 503, 504]:
        request_data.response_headers = json.dumps(dict(r.headers))
        request_data.response_body = r.content.decode()
        request_data.status_code = r.status_code
        request_data.successful_response = True
    else:
        # Handle the case where the request failed completely
        error_message = str(last_error) if last_error else 'No response from endpoint'
        request_data.response_headers = json.dumps({'Message': error_message})
        request_data.response_body = json.dumps({'Message': error_message})
        request_data.status_code = r.status_code if r is not None else 999
        request_data.successful_response = False

    return request_data


acceptable_url_schemes = ('http', 'https', 'ftp', 'ftps')


def get_qlength():
    """
    Get the length of the queue from celery. Celery returns a dictionary like so: {'queue_name': [task1, task2, ...]}
    so to get qlength we simply aggregate the length of all task lists
    """
    qlength = 0
    celery_inspector = celery_app.control.inspect()
    dict_of_queues = celery_inspector.reserved()
    if dict_of_queues and isinstance(dict_of_queues, dict):
        for k, v in dict_of_queues.items():
            qlength += len(v)

    return qlength


def _get_webhook_headers() -> dict:
    return {
        'User-Agent': 'TutorCruncher',
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {settings.tc2_shared_key}',
    }


async def _try_send_webhook(client, endpoint, loaded_payload, attempt=1, max_attempts=3):
    try:
        response = await client.post(
            endpoint.webhook_url,
            json=loaded_payload,
            headers=_get_webhook_headers(),
            timeout=settings.webhook_timeout,
        )

        if response.status_code in {200, 201, 202, 204}:
            app_logger.info('Webhook request successful for endpoint %s', endpoint.id)
            return response, 'Success'
        elif response.status_code in {500, 502, 503, 504} and attempt < max_attempts:
            app_logger.info('Retrying webhook for endpoint %s (attempt %s/%s)', endpoint.id, attempt, max_attempts)
            await asyncio.sleep(2 ** (attempt - 1))  # Exponential backoff
            return await _try_send_webhook(client, endpoint, loaded_payload, attempt + 1)
        else:
            app_logger.info('Webhook request failed for endpoint %s with status %s', endpoint.id, response.status_code)
            return response, 'Failed'
    except (httpx.RequestError, httpx.TimeoutException) as e:
        if attempt < max_attempts:
            app_logger.info('Retrying webhook for endpoint %s (attempt %s/%s)', endpoint.id, attempt, max_attempts)
            await asyncio.sleep(2 ** (attempt - 1))  # Exponential backoff
            return await _try_send_webhook(client, endpoint, loaded_payload, attempt + 1)
        app_logger.error('Error sending webhook to %s: %s', endpoint.webhook_url, str(e))
        return None, 'Failed'


async def _async_post_webhooks(endpoints, url_extension, payload):
    webhook_logs = []
    total_success, total_failed = 0, 0

    try:
        loaded_payload = json.loads(payload)
        if isinstance(loaded_payload, str):
            loaded_payload = json.loads(loaded_payload)
    except json.JSONDecodeError:
        app_logger.error('Invalid JSON payload: %s', payload)
        return [], total_success, total_failed

    if not isinstance(loaded_payload, dict):
        app_logger.error('Invalid payload format: expected dictionary')
        return [], total_success, total_failed

    if loaded_payload == {}:
        app_logger.error('Empty payload')
        return [], total_success, total_failed

    # Extract branch IDs from payload
    target_branch_ids = set()

    # If branch_id is explicitly set in the payload, it overrides any branch IDs in events
    if 'branch_id' in loaded_payload:
        target_branch_ids = {loaded_payload['branch_id']}
    # Otherwise, look for branch IDs in events
    elif 'events' in loaded_payload:
        if not isinstance(loaded_payload['events'], list):
            app_logger.error('Invalid events format: expected array')
            return [], total_success, total_failed
        for event in loaded_payload['events']:
            if isinstance(event, dict) and 'branch' in event:
                target_branch_ids.add(event['branch'])

    # Only default to branch 99 if no branch information is present in the payload
    if not target_branch_ids:
        app_logger.info('No branch IDs found in payload, defaulting to branch 99')
        target_branch_ids.add(99)

    # Filter and validate endpoints
    valid_endpoints = {}  # Use dict to deduplicate by URL
    for endpoint in endpoints:
        # Skip inactive endpoints
        if not endpoint.active:
            app_logger.info('Skipping inactive endpoint: %s', endpoint.id)
            continue

        # Skip endpoints with invalid URLs
        if not endpoint.webhook_url:
            app_logger.error('Skipping endpoint with no URL: %s', endpoint.id)
            continue

        # Validate URL scheme
        if not any(endpoint.webhook_url.startswith(scheme + '://') for scheme in acceptable_url_schemes):
            app_logger.error(
                'Skipping endpoint with invalid URL scheme: %s (%s)',
                endpoint.webhook_url,
                endpoint.id,
            )
            continue

        # Check branch ID - only send if endpoint's branch matches a target branch
        if endpoint.branch_id in target_branch_ids:
            # Deduplicate by URL - only keep one endpoint per URL
            valid_endpoints[endpoint.webhook_url] = endpoint
            app_logger.info('Adding endpoint %s for branch %s', endpoint.id, endpoint.branch_id)
        else:
            app_logger.info(
                'Skipping endpoint %s for branch %s (not in target branches %s)',
                endpoint.id,
                endpoint.branch_id,
                target_branch_ids,
            )

    if not valid_endpoints:
        app_logger.info('No valid endpoints found for branch IDs: %s', target_branch_ids)
        return [], total_success, total_failed

    # Create a connection pool for all requests
    async with AsyncClient(limits=limits) as client:
        tasks = []
        for endpoint in valid_endpoints.values():
            task = asyncio.create_task(_try_send_webhook(client, endpoint, loaded_payload))
            tasks.append(task)

        # Wait for all tasks to complete
        results = await asyncio.gather(*tasks, return_exceptions=True)

        for endpoint, result in zip(valid_endpoints.values(), results):
            if isinstance(result, Exception):
                app_logger.error('Error processing webhook for endpoint %s: %s', endpoint.id, str(result))
                total_failed += 1
                continue

            response, status = result
            webhook_log = WebhookLog(
                webhook_endpoint_id=endpoint.id,
                status=status,
                status_code=response.status_code if response else 999,
                request_body=json.dumps(loaded_payload),
                response_body=response.content.decode() if response else 'Error: No response',
                timestamp=datetime.now(UTC),
            )
            webhook_logs.append(webhook_log)

            if status == 'Success':
                total_success += 1
            else:
                total_failed += 1

    return webhook_logs, total_success, total_failed


@celery_app.task
def task_send_webhooks(
    payload: str,
    url_extension: str = None,
):
    """
    Send the webhook to the relevant endpoints
    """
    try:
        loaded_payload = json.loads(payload)
        if isinstance(loaded_payload, str):
            loaded_payload = json.loads(loaded_payload)
    except json.JSONDecodeError:
        app_logger.error('Invalid JSON payload: %s', payload)
        return

    # Handle request_time field if present
    if '_request_time' in loaded_payload:
        loaded_payload['request_time'] = loaded_payload.pop('_request_time')

    with Session(engine) as db:
        endpoints = db.exec(select(WebhookEndpoint)).all()
        webhook_logs, total_success, total_failed = asyncio.run(_async_post_webhooks(endpoints, url_extension, payload))
        if webhook_logs:
            for log in webhook_logs:
                db.add(log)
            db.commit()

    app_logger.info(
        '%s Webhooks sent. Total Sent: %s. Total failed: %s',
        total_success + total_failed,
        total_success,
        total_failed,
    )
    return total_success + total_failed


DELETE_JOBS_KEY = 'delete_old_logs_job'

scheduler = AsyncIOScheduler(timezone=UTC)


@asynccontextmanager
async def lifespan(app: FastAPI):
    scheduler.start()
    yield
    scheduler.shutdown()


@scheduler.scheduled_job('interval', hours=1)
async def delete_old_logs_job():
    """
    We run cron job at midnight every day that wipes all WebhookLogs older than 15 days
    """
    if cache.get(DELETE_JOBS_KEY):
        return
    else:
        cache.set(DELETE_JOBS_KEY, 'True', ex=1200)
        _delete_old_logs_job.delay()


def get_count(date_to_delete_before: datetime) -> int:
    """
    Get the count of all logs
    """
    with Session(engine) as db:
        count = (
            db.query(WebhookLog)
            .with_entities(func.count())
            .where(
                WebhookLog.timestamp < date_to_delete_before - timedelta(minutes=1)
            )  # Add 1 minute buffer to ensure we keep 14.99-day-old logs
            .scalar()
        )
    return count


@celery_app.task
def _delete_old_logs_job():
    """Delete webhook logs older than 15 days"""
    with Session(engine) as db:
        # Calculate the cutoff date (15 days ago)
        now = datetime.now(UTC)
        cutoff_date = now - timedelta(days=15)

        # Delete logs older than 15 days (not including logs exactly 15 days old)
        # Use a small buffer to ensure we keep logs that are just under 15 days old
        stmt = delete(WebhookLog).where(WebhookLog.timestamp < cutoff_date - timedelta(minutes=1))
        result = db.execute(stmt)
        db.commit()

        # Log the operation
        app_logger.info('Deleting webhook logs older than %s', cutoff_date)
        app_logger.info('Deleted %s old webhook logs', result.rowcount)

        return result.rowcount

    cache.delete(DELETE_JOBS_KEY)
